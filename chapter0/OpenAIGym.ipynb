{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Hands on Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # to plot histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym # import the gym package\n",
    "env = gym.make('FrozenLake-v0') # load the env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reset the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset the env and returns the start state\n",
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To render an environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the number of states/actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the integer values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n)\n",
    "print(env.observation_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `FrozenLake-v0` you can also use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(env.env.nA)\n",
    "print(env.env.nS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To let our agent executes an action $a$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next state we reached: 4\n",
      "immediate reward collected: 0.0\n",
      "Is this a terminal state? False\n",
      "Probability that we actually execute action a: {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "# execute action 2 in state 0 (because we reset the environment)\n",
    "next_state, reward, terminate, probability = env.step(2)\n",
    "print(\"next state we reached:\", next_state)\n",
    "print(\"immediate reward collected:\", reward)\n",
    "print(\"Is this a terminal state?\", terminate)\n",
    "print(\"Probability that we actually execute action a:\", probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To randomly sample an action from the set of actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = env.action_space.sample()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 24, 21, 27]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADE1JREFUeJzt3W+oXHedx/HPxyaLYgtWMsbQNt4VihDFTcslFivSXf+Q\nrWIVRCxYy6JcFZUWBal94J9nPtAqu4hytcEsWytC61pKVWItFEGrN91smzZqi1RsiM2tRVtRkLQf\nH8yJXG/vZM78uXfOfPt+wXBnzjmT+fILvDk5M3PjJAIAzL/nzXoAAMB0EHQAKIKgA0ARBB0AiiDo\nAFAEQQeAIgg6ABRB0AGgCIIOAEVs28oX27FjRxYWFrbyJQFg7h0+fPjxJL1hx21p0BcWFrSysrKV\nLwkAc8/2b9ocxyUXACiCoANAEQQdAIog6ABQBEEHgCIIOgAUQdABoAiCDgBFEHQAKGJLvyk6keXl\nWU8wmqWlWU8A4DmGM3QAKIKgA0ARBB0AiiDoAFAEQQeAIgg6ABQxPx9bBIC1+Cjzs3CGDgBFEHQA\nKIKgA0ARBB0AiiDoAFAEQQeAIgg6ABRB0AGgCIIOAEUQdAAoYmjQbV9g+y7bD9p+wPY1zfbP2D5u\n+0hzu3zzxwUADNLmd7mckvTxJPfaPkfSYduHmn1fTPL5zRsPANDW0KAnOSHpRHP/KdvHJJ232YMB\nAEYz0jV02wuSLpJ0T7Ppo7bvs33A9rlTng0AMILWQbd9tqRbJF2b5ElJX5H0ckl71T+D/8KA5y3Z\nXrG9srq6OoWRAQAbaRV029vVj/lNSW6VpCSPJXk6yTOSviZp30bPTbKcZDHJYq/Xm9bcAIB12nzK\nxZJulHQsyQ1rtu9ac9g7JB2d/ngAgLbafMrlUklXSbrf9pFm2/WSrrS9V1IkPSLpA5syIQCglTaf\ncvmxJG+w647pjwMAGBffFAWAIgg6ABTR5ho6quN/TwdK4AwdAIog6ABQBEEHgCIIOgAUQdABoAiC\nDgBFEHQAKIKgA0ARBB0AiiDoAFAEQQeAIgg6ABRB0AGgCIIOAEUQdAAogqADQBEEHQCKIOgAUARB\nB4AiCDoAFEHQAaCIbbMeAChteXnWE4xmaWnWE2ACnKEDQBEEHQCKIOgAUMTQoNu+wPZdth+0/YDt\na5rtL7Z9yPZDzc9zN39cAMAgbc7QT0n6eJI9ki6R9GHbeyRdJ+nOJBdKurN5DACYkaFBT3Iiyb3N\n/ackHZN0nqQrJB1sDjso6e2bNSQAYLiRrqHbXpB0kaR7JO1McqLZ9TtJO6c6GQBgJK2DbvtsSbdI\nujbJk2v3JYmkDHjeku0V2yurq6sTDQsAGKxV0G1vVz/mNyW5tdn8mO1dzf5dkk5u9Nwky0kWkyz2\ner1pzAwA2ECbT7lY0o2SjiW5Yc2u2yRd3dy/WtJ3pz8eAKCtNl/9v1TSVZLut32k2Xa9pM9J+rbt\n90n6jaR3bc6IAIA2hgY9yY8lecDuN0x3HADAuPimKAAUQdABoAiCDgBFEHQAKIKgA0ARBB0AiiDo\nAFAEQQeAIgg6ABRB0AGgCIIOAEUQdAAogqADQBEEHQCKIOgAUARBB4AiCDoAFEHQAaAIgg4ARRB0\nACiCoANAEQQdAIog6ABQBEEHgCIIOgAUQdABoAiCDgBFEHQAKGJo0G0fsH3S9tE12z5j+7jtI83t\n8s0dEwAwTJsz9G9I2r/B9i8m2dvc7pjuWACAUQ0NepK7JT2xBbMAACYwyTX0j9q+r7kkc+7UJgIA\njGXcoH9F0ssl7ZV0QtIXBh1oe8n2iu2V1dXVMV8OADDMWEFP8liSp5M8I+lrkvad4djlJItJFnu9\n3rhzAgCGGCvotnetefgOSUcHHQsA2Brbhh1g+2ZJl0naYftRSZ+WdJntvZIi6RFJH9jEGQEALQwN\nepIrN9h84ybMAgCYAN8UBYAiCDoAFEHQAaAIgg4ARRB0ACiCoANAEQQdAIog6ABQBEEHgCIIOgAU\nQdABoAiCDgBFEHQAKIKgA0ARBB0AiiDoAFAEQQeAIgg6ABRB0AGgCIIOAEUQdAAogqADQBEEHQCK\nIOgAUARBB4AiCDoAFEHQAaAIgg4ARQwNuu0Dtk/aPrpm24ttH7L9UPPz3M0dEwAwTJsz9G9I2r9u\n23WS7kxyoaQ7m8cAgBkaGvQkd0t6Yt3mKyQdbO4flPT2Kc8FABjRuNfQdyY50dz/naSdU5oHADCm\nid8UTRJJGbTf9pLtFdsrq6urk74cAGCAcYP+mO1dktT8PDnowCTLSRaTLPZ6vTFfDgAwzLhBv03S\n1c39qyV9dzrjAADG1eZjizdL+omkV9h+1Pb7JH1O0ptsPyTpjc1jAMAMbRt2QJIrB+x6w5RnAQBM\ngG+KAkARBB0AiiDoAFAEQQeAIgg6ABRB0AGgCIIOAEUQdAAogqADQBEEHQCKIOgAUARBB4AiCDoA\nFEHQAaAIgg4ARRB0ACiCoANAEQQdAIog6ABQBEEHgCIIOgAUQdABoAiCDgBFEHQAKIKgA0ARBB0A\niiDoAFAEQQeAIrZN8mTbj0h6StLTkk4lWZzGUACA0U0U9Ma/Jnl8Cn8OAGACXHIBgCImDXok/dD2\nYdtL0xgIADCeSS+5vC7JcdsvkXTI9i+S3L32gCb0S5K0e/fuCV8OADDIRGfoSY43P09K+o6kfRsc\ns5xkMclir9eb5OUAAGcwdtBtv9D2OafvS3qzpKPTGgwAMJpJLrnslPQd26f/nG8m+f5UpgIAjGzs\noCf5taR/meIsAIAJ8LFFACiCoANAEQQdAIog6ABQBEEHgCIIOgAUQdABoAiCDgBFEHQAKIKgA0AR\nBB0AiiDoAFAEQQeAIgg6ABRB0AGgCIIOAEUQdAAogqADQBEEHQCKIOgAUARBB4AiCDoAFEHQAaAI\ngg4ARRB0ACiCoANAEQQdAIog6ABQxERBt73f9i9tP2z7umkNBQAY3dhBt32WpC9L+ndJeyRdaXvP\ntAYDAIxmkjP0fZIeTvLrJH+V9C1JV0xnLADAqCYJ+nmSfrvm8aPNNgDADDjJeE+03ylpf5L3N4+v\nkvSaJB9Zd9ySpKXm4Ssk/XL8cTfFDkmPz3qIluZpVmm+5p2nWaX5mneeZpW6Oe/LkvSGHbRtghc4\nLumCNY/Pb7b9gyTLkpYneJ1NZXslyeKs52hjnmaV5mveeZpVmq9552lWaf7mXWuSSy4/l3Sh7X+2\n/U+S3i3ptumMBQAY1dhn6ElO2f6IpB9IOkvSgSQPTG0yAMBIJrnkoiR3SLpjSrPMSmcvB21gnmaV\n5mveeZpVmq9552lWaf7m/bux3xQFAHQLX/0HgCKeE0G3fcD2SdtHB+y37f9sfoXBfbYv3uoZ180z\nbN7LbP/R9pHm9qmtnnHNLBfYvsv2g7YfsH3NBsd0Yn1bztqltX2+7Z/Z/v9m3s9ucExX1rbNrJ1Z\n22aes2z/n+3bN9jXiXUdWZLyN0mvl3SxpKMD9l8u6XuSLOkSSfd0fN7LJN0+63VtZtkl6eLm/jmS\nfiVpTxfXt+WsXVpbSzq7ub9d0j2SLuno2raZtTNr28zzMUnf3GimrqzrqLfnxBl6krslPXGGQ66Q\n9N/p+6mkF9netTXTPVuLeTsjyYkk9zb3n5J0TM/+xnAn1rflrJ3RrNefmofbm9v6N726srZtZu0M\n2+dLeoukrw84pBPrOqrnRNBbmMdfY/Da5p+C37P9ylkPI0m2FyRdpP7Z2VqdW98zzCp1aG2bywJH\nJJ2UdChJZ9e2xaxSd9b2S5I+IemZAfs7s66jIOjz6V5Ju5O8WtJ/SfrfGc8j22dLukXStUmenPU8\nZzJk1k6tbZKnk+xV/5vY+2y/apbznEmLWTuxtrbfKulkksOzeP3NRND7Wv0ag65I8uTpf96m/12A\n7bZ3zGoe29vVD+RNSW7d4JDOrO+wWbu2tqcl+YOkuyTtX7erM2t72qBZO7S2l0p6m+1H1P8tsf9m\n+3/WHdO5dW2DoPfdJum9zTvbl0j6Y5ITsx5qENsvte3m/j71/x5/P6NZLOlGSceS3DDgsE6sb5tZ\nO7a2Pdsvau6/QNKbJP1i3WFdWduhs3ZlbZN8Msn5SRbU/5UlP0rynnWHdWJdRzXRN0Xnhe2b1X+H\nfYftRyV9Wv03bZTkq+p/2/VySQ9L+rOk/5jNpH0t5n2npA/ZPiXpL5Leneat+Rm4VNJVku5vrp9K\n0vWSdkudW982s3ZpbXdJOuj+fybzPEnfTnK77Q+umbcra9tm1i6t7bN0dF1HwjdFAaAILrkAQBEE\nHQCKIOgAUARBB4AiCDoAFEHQAaAIgg4ARRB0ACjib3MsNaxslbEjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19ab3a81940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample 100 actions randomly and display the histogram\n",
    "res = [0] * env.action_space.n\n",
    "for _ in range(100):\n",
    "    res[env.action_space.sample()] += 1\n",
    "print(res)\n",
    "_ = plt.bar([1,2,3,4], res, width=0.5, color='r', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve all the informations available in the\n",
    "action-state $(s,a)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 4, 0.0, False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example with state=0, action=0\n",
    "env.env.P[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 13, 0.0, False),\n",
       " (0.3333333333333333, 14, 0.0, False),\n",
       " (0.3333333333333333, 15, 1.0, True)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another example with state=14, action=1\n",
    "env.env.P[14][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each value in the array, we can read it as follow:\n",
    "When we execute action $1$ in state $14$:\n",
    "- with probability $1/3$ we end in state $13$ and we get an immediate reward of $0.0$. $13$ is not a terminal state (False)\n",
    "- with probability $1/3$ we end in state $14$ and we get an immediate reward of $0.0$. $13$ is not a terminal state (False)\n",
    "- with probability $1/3$ we end in state $15$ and we get an immediate reward of $1.0$. $13$ is not a terminal state (True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Customize the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load and register a new environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='Deterministic-4x4-FrozenLake-v0', # name given to this new environment\n",
    "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', # env entry point\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': False} # argument passed to the env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load this newly environment using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Deterministic-4x4-FrozenLake-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see that the tiles are non slippery we can check that, now, the probability is $1.0$ (probability that our agent actually executes the action we tell him to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 0, 0.0, False)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example check it for state-action pair (0,0)\n",
    "env.P[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the $8 \\times 8$ map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we specified 'map_name': '8x8'\n",
    "register(\n",
    "    id='Deterministic-8x8-FrozenLake-v0',\n",
    "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '8x8', 'is_slippery': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "# we can see that our new environment is a 8x8 grid\n",
    "env = gym.make(\"Deterministic-8x8-FrozenLake-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define our own grid, we just need to define an array and pass it for the key 'desc' (desc means description):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_desc = [\n",
    "    \"SFFFF\",\n",
    "    \"FHFHH\",\n",
    "    \"FHFFH\",\n",
    "    \"HHFFG\"\n",
    "]\n",
    "\n",
    "register(\n",
    "    id='Stochastic-5x5-FrozenLake-v0',\n",
    "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "    kwargs={'desc': my_desc, 'is_slippery': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFF\n",
      "FHFHH\n",
      "FHFFH\n",
      "HHFFG\n"
     ]
    }
   ],
   "source": [
    "# We can check that our environment is our own 5x5 grid\n",
    "env = gym.make('Stochastic-5x5-FrozenLake-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To customize even more our environment (change the value of the reward, the transition probabilities...) we can create a class that inherits from our environment (here `gym.envs.toy_text.frozen_lake.FrozenLakeEnv`). Our class should be defined inside a python file (take a look at the file `my_env.py`). Once our class is created we just need to pass it to the `entry_point` parameter of the `register` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import my_env # import my python file that contains CustomizedFrozenLake class that inherits from FrozenLake\n",
    "\n",
    "register(\n",
    "    id='Stochastic-8x8-CustomizedFrozenLake-v0',\n",
    "    entry_point='my_env:CustomizedFrozenLake',\n",
    "    kwargs={'map_name': '8x8', 'is_slippery': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Stochastic-8x8-CustomizedFrozenLake-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the reward effectively changed (it should be 10 in the goal state and -5 in the reward state), we can just, for example, display the state just before the final state (state 62). The final state (state 63 is the goal state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 61, 0, False),\n",
       " (0.3333333333333333, 62, 0, False),\n",
       " (0.3333333333333333, 63, 10, True)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see that, if we reach state 63, we receive reward=10\n",
    "env.P[62][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
