{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands Reinforcement Learning [Part 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning and Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym # useful to load the FrozenLake environment\n",
    "import numpy as np # useful to use the random.uniform() function\n",
    "import time # useful to measure the training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "        id='Deterministic-4x4-FrozenLake-v0',\n",
    "        entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "        kwargs={'map_name': '4x4', 'is_slippery': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "4\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('Deterministic-4x4-FrozenLake-v0') # load the environment\n",
    "state = env.reset() # reset the environment and return the starting state\n",
    "env.render() # render the environment\n",
    "print()\n",
    "print(env.action_space.n) # display the number of actions: 4\n",
    "print(env.observation_space.n) # display the number of states: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, s, epsilon):\n",
    "    p = np.random.uniform()\n",
    "    if p < epsilon:\n",
    "        # the sample() method from the environment allows\n",
    "        # to randomly sample an action from the set of actions\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        # act greedily by selecting the best action possible in the current state\n",
    "        return np.argmax(Q[s, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize our Q-table: matrix of size [n_states, n_actions] with zeros\n",
    "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "# set the hyperparameters\n",
    "epsilon = 0.1 # epsilon value for the epsilon greedy strategy\n",
    "lr = 0.8 # learning rate\n",
    "gamma = 0.95 # discount factor\n",
    "episodes = 10000 # number of episode\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    terminate = False # did the game end ?\n",
    "    while True:\n",
    "        # choose an action using the epsilon greedy strategy\n",
    "        action = epsilon_greedy(Q, state, epsilon)\n",
    "\n",
    "        # execute the action. The environment provides us\n",
    "        # 4 values: \n",
    "        # - the next_state we ended in after executing our action\n",
    "        # - the reward we get from executing that action\n",
    "        # - wether or not the game ended\n",
    "        # - the probability of executing our action \n",
    "        # (we don't use this information here)\n",
    "        next_state, reward, terminate, _ = env.step(action)\n",
    "\n",
    "        if reward == 0: # if we didn't reach the goal state\n",
    "            if terminate: # if the agent falls in an hole\n",
    "                r = -5 # then give them a big negative reward\n",
    "\n",
    "                # the Q-value of the terminal state equals the reward\n",
    "                Q[next_state] = np.ones(n_actions) * r\n",
    "            else: # the agent is in a frozen tile\n",
    "                r = -1 # give the agent a little negative reward to avoid long episode\n",
    "        if reward == 1: # the agent reach the goal state\n",
    "            r = 100 # give him a big reward\n",
    "\n",
    "            # the Q-value of the terminal state equals the reward\n",
    "            Q[next_state] = np.ones(n_actions) * r\n",
    "\n",
    "        # Q-learning update\n",
    "        Q[state,action] = Q[state,action] + lr * (r + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "\n",
    "        # move the agent to the new state before executing the next iteration\n",
    "        state = next_state\n",
    "\n",
    "        # if we reach the goal state or fall in an hole\n",
    "        # end the current episode\n",
    "        if terminate:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.79458602   4.23867511  10.19140003   5.16406954]\n",
      " [ -5.5974447   -8.86780938  -9.74674068  -6.49789267]\n",
      " [ -8.83413887  -7.0899909   -8.80651155  -7.35387124]\n",
      " [ -7.65416181  -7.5385439  -10.30876204 -10.97979487]\n",
      " [  9.76670888  -9.60220376  -6.70756983  -5.68055684]\n",
      " [ -5.          -5.          -5.          -5.        ]\n",
      " [ -8.95632286  -9.13790343  -9.92329327  -9.74982789]\n",
      " [ -5.          -5.          -5.          -5.        ]\n",
      " [ 13.71821499  -6.37679686  -1.17976064  21.18053432]\n",
      " [ -5.17266067 127.8232648   -8.85055552   4.29425646]\n",
      " [ 30.03089622  -6.13372615  -9.70804677  -8.82228622]\n",
      " [ -5.          -5.          -5.          -5.        ]\n",
      " [ -5.          -5.          -5.          -5.        ]\n",
      " [  2.21208633  24.21894155 125.05735876  12.94292584]\n",
      " [ 31.34126215 121.9976938   89.5078087   54.6152641 ]\n",
      " [100.         100.         100.         100.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "####################\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "####################\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "####################\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "####################\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "####################\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "state = env.reset() # reinitialize the environment\n",
    "while True:\n",
    "    # once the agent has been trained, it\n",
    "    # will take the best action in each state\n",
    "    action = np.argmax(Q[state,:])\n",
    "\n",
    "    # execute the action and recover a tuple of values\n",
    "    next_state, reward, terminate, _ = env.step(action)\n",
    "    print(\"####################\")\n",
    "    env.render() # display the new state of the game\n",
    "\n",
    "    # move the agent to the new state before executing the next iteration\n",
    "    state = next_state\n",
    "\n",
    "    # if the agent falls in an gole or ends in the goal state\n",
    "    if terminate:\n",
    "        break # break out of the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning and Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "4\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "print()\n",
    "print(env.action_space.n) \n",
    "print(env.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reset graph. Usefull when one uses a Jupyter notebook and\n",
    "# has already executed the cell that creates the TensorFlow graph.\n",
    "tf.reset_default_graph() \n",
    "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "\n",
    "# input states\n",
    "inputs = tf.placeholder(dtype=tf.float32, shape=[None, n_states])\n",
    "\n",
    "# parameter of our neural-network\n",
    "W = tf.get_variable(dtype=tf.float32, shape=[n_states, n_actions],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                    name='W')\n",
    "b = tf.get_variable(dtype=tf.float32, shape=[n_actions], \n",
    "                    initializer=tf.zeros_initializer(),\n",
    "                    name=\"b\")\n",
    "\n",
    "Q_pred = tf.matmul(inputs, W) + b\n",
    "a_pred = tf.argmax(Q_pred, 1) # predicted action\n",
    "\n",
    "# Q_target will be computed according to equation (2)\n",
    "Q_target = tf.placeholder(dtype=tf.float32, shape=[1, n_actions])\n",
    "\n",
    "# compute the loss according to equation (1)\n",
    "loss = tf.reduce_sum(tf.square(Q_target - Q_pred))\n",
    "\n",
    "# define the update rule for our network\n",
    "update = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 750 800 850 900 950 1000 1050 1100 1150 1200 1250 1300 1350 1400 1450 1500 1550 1600 1650 1700 1750 1800 1850 1900 1950 \n",
      "###################\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "###################\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# parameters\n",
    "gamma = 0.95\n",
    "epsilon = 0.1\n",
    "episodes = 2000\n",
    "\n",
    "# initialize the TensorFlow session and train the model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize the variables of our model (a.k.a parameters W and b)\n",
    "    for episode in range(episodes):\n",
    "        if episode % 50 == 0:\n",
    "            print(episode, end=\" \")\n",
    "        state = env.reset() # reset environment and get initial state\n",
    "        r_total = 0 # sum of reward in current episode\n",
    "        while True:\n",
    "            # create the onehot vector associate to the state 'state':\n",
    "            input_state = np.identity(n_states)[state:state+1]\n",
    "\n",
    "            # recover the value of Q_pred and a_pred from the neural-network\n",
    "            apred, Qpred = sess.run([a_pred, Q_pred], feed_dict={inputs: input_state})\n",
    "\n",
    "            # use epsilon-greedy strategy\n",
    "            if np.random.uniform() < epsilon:\n",
    "                # if we explore, overide the action returned by the neural-network\n",
    "                # with a random action\n",
    "                apred[0] = env.action_space.sample()\n",
    "\n",
    "            # get next state, reward and if the game ended or not\n",
    "            next_state, reward, terminate, _ = env.step(apred[0])\n",
    "\n",
    "            # reuse the same code as in Q-learning to negate reward\n",
    "            if r == 0:\n",
    "                if t == True:\n",
    "                    r = -10\n",
    "                else:\n",
    "                    r = -1\n",
    "            \n",
    "            if r == 1:\n",
    "                r = 100\n",
    "\n",
    "            # obtain the Q(s', a') from equation (2) value by feeding the new state in our neural-network\n",
    "            input_next_state = np.identity(n_states)[next_state:next_state+1]\n",
    "            Qpred_next = sess.run(Q_pred, feed_dict={inputs: input_next_state})\n",
    "\n",
    "            # the the max of Qpred_next = Q(s', a') over a'\n",
    "            Qmax = np.max(Qpred_next)\n",
    "\n",
    "            # update Q(s,a)_target from equation (2)\n",
    "            Qtarget = Qpred\n",
    "            Qtarget[0, apred[0]] = r + gamma * Qmax\n",
    "\n",
    "            # Train the neural-network using the Qtarget and Qpred and the update rule\n",
    "            loss = sess.run(update, feed_dict={inputs: input_state, Q_target: Qtarget})\n",
    "\n",
    "            r_total += r\n",
    "            \n",
    "            # move to next_state before next iteration\n",
    "            state = next_state\n",
    "            if terminate: # end episode if agent falls in hole or goal state has been reached\n",
    "                break\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    s = env.reset()\n",
    "    while True:\n",
    "        input_state = np.identity(n_states)[s:s+1]\n",
    "        a = sess.run(a_pred, feed_dict={inputs: input_state})\n",
    "        next_s, r, terminate, _ = env.step(a[0])\n",
    "        print(\"###################\")\n",
    "        env.render()\n",
    "        s = next_s\n",
    "        if terminate:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
